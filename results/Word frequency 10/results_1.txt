Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='ArtPainting')
Sources Cartoon, Photo, Sketch
Model of Cartoon loaded 
Model of Photo loaded 
Model of Sketch loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2344/2344 [01:25<00:00, 27.32it/s]
100% 1670/1670 [01:00<00:00, 27.50it/s]
100% 3929/3929 [02:23<00:00, 27.39it/s]
Evaluation on the Target domain - ArtPainting
100% 2048/2048 [01:53<00:00, 17.97it/s]
Accuracy mean: 67.63 
Accuracy text_domain_embedding: 71.53 

Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='Cartoon')
Sources ArtPainting, Photo, Sketch
Model of ArtPainting loaded 
Model of Photo loaded 
Model of Sketch loaded 
Downloading: "https://download.pytorch.org/models/resnet101-5d3b4d8f.pth" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth
100% 170M/170M [00:05<00:00, 30.7MB/s]
Downloading: 100% 455k/455k [00:00<00:00, 3.12MB/s]
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2048/2048 [01:15<00:00, 27.23it/s]
100% 1670/1670 [01:01<00:00, 27.21it/s]
100% 3929/3929 [02:22<00:00, 27.51it/s]
Evaluation on the Target domain - Cartoon
100% 2344/2344 [02:11<00:00, 17.86it/s]
Accuracy mean: 57.12 
Accuracy text_domain_embedding: 57.76

Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='Sketch')
Sources ArtPainting, Cartoon, Photo
Model of ArtPainting loaded 
Model of Cartoon loaded 
Model of Photo loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2048/2048 [01:15<00:00, 27.16it/s]
100% 2344/2344 [01:26<00:00, 27.21it/s]
100% 1670/1670 [01:01<00:00, 27.19it/s]
Evaluation on the Target domain - Sketch
100% 3929/3929 [03:41<00:00, 17.77it/s]
Accuracy mean: 60.40 
Accuracy text_domain_embedding: 62.46

Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='Photo')
Sources ArtPainting, Cartoon, Sketch
Model of ArtPainting loaded 
Model of Cartoon loaded 
Model of Sketch loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2048/2048 [01:14<00:00, 27.55it/s]
100% 2344/2344 [01:25<00:00, 27.44it/s]
100% 3929/3929 [02:23<00:00, 27.43it/s]
Evaluation on the Target domain - Photo
100% 1670/1670 [01:33<00:00, 17.81it/s]
Accuracy mean: 94.49 
Accuracy text_domain_embedding: 95.69