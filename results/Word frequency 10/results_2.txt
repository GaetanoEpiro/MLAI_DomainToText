Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='ArtPainting')
Sources Cartoon, Photo, Sketch
Model of Cartoon loaded 
Model of Photo loaded 
Model of Sketch loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2344/2344 [01:27<00:00, 26.76it/s]
100% 1670/1670 [01:02<00:00, 26.77it/s]
100% 3929/3929 [02:25<00:00, 26.95it/s]
Evaluation on the Target domain - ArtPainting
100% 2048/2048 [01:57<00:00, 17.48it/s]
Accuracy mean: 67.63 
Accuracy text_domain_embedding: 69.92 

Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='Cartoon')
Sources ArtPainting, Photo, Sketch
Model of ArtPainting loaded 
Model of Photo loaded 
Model of Sketch loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2048/2048 [01:15<00:00, 27.18it/s]
100% 1670/1670 [01:01<00:00, 27.10it/s]
100% 3929/3929 [02:22<00:00, 27.54it/s]
Evaluation on the Target domain - Cartoon
100% 2344/2344 [02:09<00:00, 18.07it/s]
Accuracy mean: 57.12 
Accuracy text_domain_embedding: 57.25

Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='Photo')
Sources ArtPainting, Cartoon, Sketch
Model of ArtPainting loaded 
Model of Cartoon loaded 
Model of Sketch loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2048/2048 [01:13<00:00, 27.96it/s]
100% 2344/2344 [01:24<00:00, 27.90it/s]
100% 3929/3929 [02:21<00:00, 27.84it/s]
Evaluation on the Target domain - Photo
100% 1670/1670 [01:35<00:00, 17.47it/s]
Accuracy mean: 94.49 
Accuracy text_domain_embedding: 95.27

Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='Sketch')
Sources ArtPainting, Cartoon, Photo
Model of ArtPainting loaded 
Model of Cartoon loaded 
Model of Photo loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2048/2048 [01:15<00:00, 27.10it/s]
100% 2344/2344 [01:26<00:00, 27.19it/s]
100% 1670/1670 [01:02<00:00, 26.90it/s]
Evaluation on the Target domain - Sketch
100% 3929/3929 [03:41<00:00, 17.73it/s]
Accuracy mean: 60.40 
Accuracy text_domain_embedding: 62.87