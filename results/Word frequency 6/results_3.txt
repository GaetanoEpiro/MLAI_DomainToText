Generated image splits
Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='ArtPainting')
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Sources Cartoon, Photo, Sketch
Model of Cartoon loaded 
Model of Photo loaded 
Model of Sketch loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2344/2344 [00:34<00:00, 68.57it/s]
100% 1670/1670 [00:24<00:00, 67.81it/s]
100% 3929/3929 [00:56<00:00, 69.03it/s]
Evaluation on the Target domain - ArtPainting
100% 2048/2048 [00:43<00:00, 46.81it/s]
Accuracy mean: 67.63 
Accuracy text_domain_embedding: 69.73 



Generated image splits
Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='Cartoon')
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Sources ArtPainting, Photo, Sketch
Model of ArtPainting loaded 
Model of Photo loaded 
Model of Sketch loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2048/2048 [00:30<00:00, 66.43it/s]
100% 1670/1670 [00:25<00:00, 65.95it/s]
100% 3929/3929 [00:57<00:00, 68.17it/s]
Evaluation on the Target domain - Cartoon
100% 2344/2344 [00:50<00:00, 46.30it/s]
Accuracy mean: 57.12 
Accuracy text_domain_embedding: 59.77 



Generated image splits
Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='Photo')
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Sources ArtPainting, Cartoon, Sketch
Model of ArtPainting loaded 
Model of Cartoon loaded 
Model of Sketch loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2048/2048 [00:30<00:00, 67.88it/s]
100% 2344/2344 [00:34<00:00, 68.37it/s]
100% 3929/3929 [00:57<00:00, 68.30it/s]
Evaluation on the Target domain - Photo
100% 1670/1670 [00:35<00:00, 46.76it/s]
Accuracy mean: 94.49 
Accuracy text_domain_embedding: 95.39 





Generated image splits
Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='Sketch')
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Sources ArtPainting, Cartoon, Photo
Model of ArtPainting loaded 
Model of Cartoon loaded 
Model of Photo loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2048/2048 [00:30<00:00, 67.52it/s]
100% 2344/2344 [00:34<00:00, 68.37it/s]
100% 1670/1670 [00:24<00:00, 66.88it/s]
Evaluation on the Target domain - Sketch
100% 3929/3929 [01:22<00:00, 47.85it/s]
Accuracy mean: 60.40 
Accuracy text_domain_embedding: 61.39 

