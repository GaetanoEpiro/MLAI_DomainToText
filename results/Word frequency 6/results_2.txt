Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='ArtPainting')
Sources Cartoon, Photo, Sketch
Downloading: "https://download.pytorch.org/models/resnet18-5c106cde.pth" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth
100% 44.7M/44.7M [00:00<00:00, 147MB/s]
Model of Cartoon loaded 
Model of Photo loaded 
Model of Sketch loaded 
Downloading: "https://download.pytorch.org/models/resnet101-5d3b4d8f.pth" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth
100% 170M/170M [00:01<00:00, 152MB/s]
Downloading: 100% 455k/455k [00:00<00:00, 3.12MB/s]
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2344/2344 [01:25<00:00, 27.40it/s]
100% 1670/1670 [01:01<00:00, 27.33it/s]
100% 3929/3929 [02:22<00:00, 27.49it/s]
Evaluation on the Target domain - ArtPainting
100% 2048/2048 [01:54<00:00, 17.83it/s]
Accuracy mean: 67.63 
Accuracy text_domain_embedding: 71.58

Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='Cartoon')
Sources ArtPainting, Photo, Sketch
Model of ArtPainting loaded 
Model of Photo loaded 
Model of Sketch loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2048/2048 [01:15<00:00, 26.96it/s]
100% 1670/1670 [01:01<00:00, 27.19it/s]
100% 3929/3929 [02:23<00:00, 27.46it/s]
Evaluation on the Target domain - Cartoon
100% 2344/2344 [02:12<00:00, 17.71it/s]
Accuracy mean: 57.12 
Accuracy text_domain_embedding: 58.06 

Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='Sketch')
Sources ArtPainting, Cartoon, Photo
Model of ArtPainting loaded 
Model of Cartoon loaded 
Model of Photo loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2048/2048 [01:14<00:00, 27.49it/s]
100% 2344/2344 [01:25<00:00, 27.28it/s]
100% 1670/1670 [01:00<00:00, 27.58it/s]
Evaluation on the Target domain - Sketch
100% 3929/3929 [03:40<00:00, 17.85it/s]
Accuracy mean: 60.40 
Accuracy text_domain_embedding: 62.61 

Namespace(gpu=0, num_classes=7, num_test=0.25, num_training=0.6, num_validation=0.15, path_to_dataset='/content/MLAI_DomainToText/', path_to_txt='/content/MLAI_DomainToText/data_api/PACS', target='Photo')
Sources ArtPainting, Cartoon, Sketch
Model of ArtPainting loaded 
Model of Cartoon loaded 
Model of Sketch loaded 
/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Model pretrained on Textures loaded
100% 2048/2048 [01:15<00:00, 27.11it/s]
100% 2344/2344 [01:25<00:00, 27.28it/s]
100% 3929/3929 [02:22<00:00, 27.66it/s]
Evaluation on the Target domain - Photo
100% 1670/1670 [01:33<00:00, 17.86it/s]
Accuracy mean: 94.49 
Accuracy text_domain_embedding: 95.45